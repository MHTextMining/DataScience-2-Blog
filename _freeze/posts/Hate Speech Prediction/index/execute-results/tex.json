{
  "hash": "1cb9f9d9d2d8cb0fcda8d1e13b3e5f33",
  "result": {
    "markdown": "---\ntitle: \"Hate-Speech Prediction\"\nauthor: \"Markus H√§fner\"\ndate: \"02.10.2023\"\nformat:\n  pdf:\n    output-file: Hallo\n    toc: true\nbibliography: references.bib\n---\n\n\n# Introduction\n\nIn this blog post we will be solving a classification problem by trying to identify Hate-Speech\nin tweets using machine learning algorithms. \nThe data that we will be using is sourced from [@wiegand2019].\n\n# Setup\n\n### Loading Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wordcloud)\nlibrary(fastrtext)\nlibrary(tidyverse)\nlibrary(tokenizers)\nlibrary(tidytext)\nlibrary(hcandersenr)\nlibrary(SnowballC)  \nlibrary(lsa)  \nlibrary(easystats)  \nlibrary(textclean)  \nlibrary(quanteda)\nlibrary(wordcloud)\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(discrim)  \nlibrary(naivebayes)\nlibrary(tictoc)  \nlibrary(remoji)  \nlibrary(pradadata)\nlibrary(knitr)\nlibrary(parsnip)\nlibrary(purrr)\n```\n:::\n\n\n## Train - Data\n\n### Load Data\n\nWe start by reading in the training data right from text file provided by [@wiegand2019].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <- \n  data_read(\"data/germeval2018.training.txt\",\n         header = FALSE)\n\nkable(head(d_train))\n```\n\n::: {.cell-output-display}\n|V1                                                                                                                                                                                                                                                                                           |V2      |V3     |\n|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------|:------|\n|@corinnamilborn Liebe Corinna, wir w√ºrden dich gerne als Moderatorin f√ºr uns gewinnen! W√§rst du begeisterbar?                                                                                                                                                                                |OTHER   |OTHER  |\n|@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverst√§ndlich. Dass das BVerfG Sachleistungen nicht ausschlie√üt, kritisieren wir.                                                                                                                                               |OTHER   |OTHER  |\n|@ahrens_theo fr√∂hlicher gru√ü aus der sch√∂nsten stadt der welt theo ‚öìÔ∏è                                                                                                                                                                                                                        |OTHER   |OTHER  |\n|@dushanwegner Amis h√§tten alles und jeden gew√§hlt...nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!                                                                                                                                                 |OTHER   |OTHER  |\n|@spdde kein verl√§√ülicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgespr√§chen - schickt diese St√ºmper #SPD in die Versenkung.                                                                                                                                                     |OFFENSE |INSULT |\n|@Dirki_M Ja, aber wo widersprechen die Zahlen denn denen, die im von uns verlinkten Artikel stehen? In unserem Tweet geht es rein um subs. Gesch√ºtzte. 2017 ist der gesamte Familiennachzug im Vergleich zu 2016 - die Zahlen, die Hr. Brandner bem√ºht - √ºbrigens leicht r√ºckl√§ufig gewesen. |OTHER   |OTHER  |\n:::\n:::\n\n\n### Renaming Columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(d_train) <- c(\"text\", \"c1\", \"c2\")\n```\n:::\n\n\n### Adding ID Column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train_id <- d_train %>% \n  mutate(id = row_number()) \n```\n:::\n\n\n## Test - Data\n\n### Load Data\n\nSame procedure as with our training data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test <- \n  data_read(\"data/germeval2018.test.txt\",\n         header = FALSE)\n\nkable(head(d_test))\n```\n\n::: {.cell-output-display}\n|V1                                                                                                                                                                          |V2      |V3    |\n|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------|:-----|\n|Meine Mutter hat mir erz√§hlt, dass mein Vater einen Wahlkreiskandidaten nicht gew√§hlt hat, weil der gegen die Homo-Ehe ist ‚ò∫                                                |OTHER   |OTHER |\n|@Tom174_ @davidbest95 Meine Reaktion; &#124;LBR&#124; Nicht jeder Moslem ist ein Terrorist. Aber jeder Moslem glaubt an √úberlieferungen, die Gewalt und Terror beg√ºnstigen. |OTHER   |OTHER |\n|#Merkel rollt dem Emir von #Katar, der islamistischen Terror unterst√ºtzt, den roten Teppich aus.Wir brauchen einen sofortigen #Waffenstopp!                                 |OTHER   |OTHER |\n|‚ÄûMerle ist kein junges unschuldiges M√§dchen‚Äú Kch....... üò± #tatort                                                                                                          |OTHER   |OTHER |\n|@umweltundaktiv Asylantenflut bringt eben nur negatives f√ºr Deutschland. Drum Asylanenstop und R√ºckf√ºhrung der Mehrzahl.                                                    |OFFENSE |ABUSE |\n|@_StultaMundi Die Bibel enth√§lt ebenfalls Gesetze des Zivil- und Strafrechts.                                                                                               |OTHER   |OTHER |\n:::\n:::\n\n\n### Renaming Columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(d_test) <- c(\"text\", \"c1\", \"c2\")\n```\n:::\n\n\n### Adding ID Column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test_id <- d_test %>% \n  mutate(id = row_number()) \n```\n:::\n\n\n# Exploratory Data Analysis\n\n## Classifiers\n\nLets take a closer look at how our data is labeled. How many tweets are considered Hate-Speech?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkable(d_train %>% \n  filter(c1 == \"OFFENSE\") %>%\n  nrow() / nrow(d_train))\n```\n\n::: {.cell-output-display}\n|         x|\n|---------:|\n| 0.3369934|\n:::\n:::\n\n\nIt seems like  1/3 of the tweets contain Hate-Speech. and 2/3 dont. \nWe wont really be taking classifier 2 (c2) into account during our analysis, \nbut lets look at what type of Hate-Speech is the most common.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkable(d_train_id %>% \n  count(c2))\n```\n\n::: {.cell-output-display}\n|c2        |    n|\n|:---------|----:|\n|ABUSE     | 1022|\n|INSULT    |  595|\n|OTHER     | 3321|\n|PROFANITY |   71|\n:::\n:::\n\n\nEvery tweet that has been classified as `OFFENSE` is categorized in either of the four categories\n`PROFANITY`, `ABUSE`, `INSULT` or `OTHER`. \nLooking at the table we can see that most Hate-Speech cases are due to abuse.\n\n## Text\n\n### Word Frequency\n\nLets see what the 20 most used words are.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfrequent_terms <- qdap:::freq_terms(d_train$text, 100)\n\nggplot(frequent_terms[1:20,], aes(x=FREQ, y=WORD, fill=WORD))+\n  geom_bar(stat=\"identity\")+\n  theme_minimal()+\n  theme(axis.text.y = element_text(angle = 0, hjust = 1))+\n  ylab(\"\")+\n  xlab(\"Most frequently used words\")+\n  guides(fill=FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nIts not odd that the most used words are just filler words. \nWhat we can conclude from that is that it will be really important for our later prediction \nto use `stopwords`. Lets filter them out and try again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfrequent_terms_sw <- qdap:::freq_terms(d_train$text, stopwords = stopwords_de, 100)\n\nggplot(frequent_terms_sw[1:20,], aes(x=FREQ, y=WORD, fill=WORD))+\n  geom_bar(stat=\"identity\")+\n  theme_minimal()+\n  theme(axis.text.y = element_text(angle = 0, hjust = 1))+\n  ylab(\"\")+\n  xlab(\"Most frequently used words excluding stopwords\")+\n  guides(fill=FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/unnamed-chunk-11-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nLets plot a wordcloud, just because we can.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwordcloud(words = frequent_terms_sw$WORD, \n          freq = frequent_terms_sw$FREQ, \n          max.words = 100, \n          colors = bluebrown_colors())\n```\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/unnamed-chunk-12-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nBasically just cool to look at. \nEven tough we can already see that a lot of words refer to tagged users such as feldenfrizz.\n\n# Feature Engineering\n\n## Adding Text_length as Variable\n\nNice to have!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train_tl <-\n  d_train_id %>% \n  mutate(text_length = str_length(text))\n\nkable(head(d_train_tl))\n```\n\n::: {.cell-output-display}\n|text                                                                                                                                                                                                                                                                                         |c1      |c2     | id| text_length|\n|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------|:------|--:|-----------:|\n|@corinnamilborn Liebe Corinna, wir w√ºrden dich gerne als Moderatorin f√ºr uns gewinnen! W√§rst du begeisterbar?                                                                                                                                                                                |OTHER   |OTHER  |  1|         109|\n|@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverst√§ndlich. Dass das BVerfG Sachleistungen nicht ausschlie√üt, kritisieren wir.                                                                                                                                               |OTHER   |OTHER  |  2|         142|\n|@ahrens_theo fr√∂hlicher gru√ü aus der sch√∂nsten stadt der welt theo ‚öìÔ∏è                                                                                                                                                                                                                        |OTHER   |OTHER  |  3|          69|\n|@dushanwegner Amis h√§tten alles und jeden gew√§hlt...nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!                                                                                                                                                 |OTHER   |OTHER  |  4|         140|\n|@spdde kein verl√§√ülicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgespr√§chen - schickt diese St√ºmper #SPD in die Versenkung.                                                                                                                                                     |OFFENSE |INSULT |  5|         136|\n|@Dirki_M Ja, aber wo widersprechen die Zahlen denn denen, die im von uns verlinkten Artikel stehen? In unserem Tweet geht es rein um subs. Gesch√ºtzte. 2017 ist der gesamte Familiennachzug im Vergleich zu 2016 - die Zahlen, die Hr. Brandner bem√ºht - √ºbrigens leicht r√ºckl√§ufig gewesen. |OTHER   |OTHER  |  6|         284|\n:::\n:::\n\n\n## Sentiment Analysis\n\nFor our sentiments we will be using `SentiWS` by [@remus-etal-2010-sentiws]. \nThe list contains a total of 16,406 positive and 16,328 negative german word forms.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsentiments <- read_csv(\"data/sentiments.csv\")\n```\n:::\n\n\nTo apply the sentiments we first need to `tokenize` our text. \nThat way we will be able to apply a sentiment to each word.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train_unnest <-\n  d_train_tl %>% \n  unnest_tokens(input = text, output = token)\n\nkable(head(d_train_unnest))\n```\n\n::: {.cell-output-display}\n|c1    |c2    | id| text_length|token          |\n|:-----|:-----|--:|-----------:|:--------------|\n|OTHER |OTHER |  1|         109|corinnamilborn |\n|OTHER |OTHER |  1|         109|liebe          |\n|OTHER |OTHER |  1|         109|corinna        |\n|OTHER |OTHER |  1|         109|wir            |\n|OTHER |OTHER |  1|         109|w√ºrden         |\n|OTHER |OTHER |  1|         109|dich           |\n:::\n:::\n\n\nNow can combine our two data sets and match each word with its sentiment value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train_senti <- \n  d_train_unnest %>%  \n  inner_join(sentiments %>% select(-inflections), by = c(\"token\" = \"word\"))\n\nkable(head(d_train_senti))\n```\n\n::: {.cell-output-display}\n|c1    |c2    | id| text_length|token         |neg_pos |   value|\n|:-----|:-----|--:|-----------:|:-------------|:-------|-------:|\n|OTHER |OTHER |  1|         109|gewinnen      |pos     |  0.0040|\n|OTHER |OTHER |  2|         142|kritisieren   |neg     | -0.3466|\n|OTHER |OTHER |  6|         284|widersprechen |neg     | -0.3540|\n|OTHER |OTHER |  6|         284|rein          |pos     |  0.0040|\n|OTHER |OTHER |  6|         284|leicht        |pos     |  0.0040|\n|OTHER |OTHER |  6|         284|r√ºckl√§ufig    |neg     | -0.0544|\n:::\n:::\n\n\nLets take a look at our tweets again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_sentiments <-\n  d_train_senti %>% \n  group_by(id, neg_pos) %>% \n  summarise(mean = mean(value))\n```\n:::\n\n\nAnd spread the positive/negative values into their own respective columns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_sentiments_spread <-\n  train_sentiments %>% \n  pivot_wider(names_from = \"neg_pos\", values_from = \"mean\")\n\nkable(head(train_sentiments_spread))\n```\n\n::: {.cell-output-display}\n| id|    pos|     neg|\n|--:|------:|-------:|\n|  1| 0.0040|      NA|\n|  2|     NA| -0.3466|\n|  6| 0.0040| -0.2042|\n|  8|     NA| -0.5023|\n|  9| 0.5161|      NA|\n| 11| 0.0040|      NA|\n:::\n:::\n\n\nFinally lets unite our \"sentimented\" data with our original data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train_senti <-\n  d_train_tl %>% \n  full_join(train_sentiments_spread)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"id\"\n```\n:::\n\n```{.r .cell-code}\nkable(head(d_train_senti))\n```\n\n::: {.cell-output-display}\n|text                                                                                                                                                                                                                                                                                         |c1      |c2     | id| text_length|   pos|     neg|\n|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------|:------|--:|-----------:|-----:|-------:|\n|@corinnamilborn Liebe Corinna, wir w√ºrden dich gerne als Moderatorin f√ºr uns gewinnen! W√§rst du begeisterbar?                                                                                                                                                                                |OTHER   |OTHER  |  1|         109| 0.004|      NA|\n|@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverst√§ndlich. Dass das BVerfG Sachleistungen nicht ausschlie√üt, kritisieren wir.                                                                                                                                               |OTHER   |OTHER  |  2|         142|    NA| -0.3466|\n|@ahrens_theo fr√∂hlicher gru√ü aus der sch√∂nsten stadt der welt theo ‚öìÔ∏è                                                                                                                                                                                                                        |OTHER   |OTHER  |  3|          69|    NA|      NA|\n|@dushanwegner Amis h√§tten alles und jeden gew√§hlt...nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!                                                                                                                                                 |OTHER   |OTHER  |  4|         140|    NA|      NA|\n|@spdde kein verl√§√ülicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgespr√§chen - schickt diese St√ºmper #SPD in die Versenkung.                                                                                                                                                     |OFFENSE |INSULT |  5|         136|    NA|      NA|\n|@Dirki_M Ja, aber wo widersprechen die Zahlen denn denen, die im von uns verlinkten Artikel stehen? In unserem Tweet geht es rein um subs. Gesch√ºtzte. 2017 ist der gesamte Familiennachzug im Vergleich zu 2016 - die Zahlen, die Hr. Brandner bem√ºht - √ºbrigens leicht r√ºckl√§ufig gewesen. |OTHER   |OTHER  |  6|         284| 0.004| -0.2042|\n:::\n:::\n\n\n## Profanities\n\nTo create a list of `profanities` we are going to combine data from three different sources.\n\n\\(1\\) A publicly available list of over 6000 German profane words [@schimpfw].\n\n\\(2\\) `schimpfwoerter` by [@pradadata] provides another list of profane German words.\n\n\\(3\\) [@ahn] curated a list of 1,300+ English terms that could be found offensive. \nEven though our tweets are in German, we are going to give it a try, \nsince nowadays a lot of people are using English words or even a mixes of English and German words.\n\n### Load/Rename Lists\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprofanities1 <- \n  data_read(\"data/profanities.txt\",\n         header = FALSE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n profanities2 <- \n   schimpfwoerter %>% \n   mutate_all(str_to_lower) %>% \n   rename(V1 = \"word\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprofanities3 <- \n  data_read(\"data/profanities_en.txt\",\n         header = FALSE)\n```\n:::\n\n\n### Merge Lists\n\nWe are applying the function distinct() to remove duplicates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprofanities <-\n  profanities1 %>% \n  bind_rows(profanities2) %>%\n  bind_rows(profanities3) %>%\n  distinct()\n\nkable(nrow(profanities))\n```\n\n::: {.cell-output-display}\n|    x|\n|----:|\n| 7572|\n:::\n:::\n\n\nTokenizing and applying our curated profanity list.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train_prof <- \nd_train_unnest %>% \n  select(id, token) %>% \n  mutate(profanity = token %in% profanities$V1)\n```\n:::\n\n\nHow many words are considered profane?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkable(d_train_prof %>% \n  count(profanity))\n```\n\n::: {.cell-output-display}\n|profanity |     n|\n|:---------|-----:|\n|FALSE     | 96170|\n|TRUE      |  4047|\n:::\n:::\n\n\nIt seems like about one third of our total words are considered as profane. \nThis seems a bit high. \nLets check our results to see if anything went wrong.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train_prof %>% arrange(desc(profanity), .by_group = TRUE)\n```\n:::\n\n\nOur mistake is obvious. \nThe German word `die` is considered profane, since it means `stirb` in English.\nLets remove `die` from our list and try again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprofanities <- subset(profanities, V1!= \"die\") \n```\n:::\n\n\nNext try. How many words are considered profane?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train_prof <- \nd_train_unnest %>% \n  select(id, token) %>% \n  mutate(profanity = token %in% profanities$V1)\n\nkable(d_train_prof %>% \n  count(profanity))\n```\n\n::: {.cell-output-display}\n|profanity |     n|\n|:---------|-----:|\n|FALSE     | 98834|\n|TRUE      |  1383|\n:::\n:::\n\n\nThat sounds like a reasonable amount! \nNow we can combine our results with our main data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train2 <-\n  d_train_senti %>% \n  full_join(d_train_prof)\n```\n:::\n\n\n## Emojis\n\nFor emojis we source the emoji data list from the package [@remoji], It\nincludes a total of 870 emojis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemojis <- emoji(list_emoji(), pad = FALSE)\n```\n:::\n\n\nSince a sole list of emojis is not really much use for predicting Hate-Speech,\nit would be useful to have the corresponding sentiment score of each emoji.\nLuckily [@Kralj2015emojis] provides the needed information on their [website](https://kt.ijs.si/data/Emoji_sentiment_ranking/index.html).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemojis_sentiments <- data_read(\"data/emoji_sentiment_scores.csv\")\n```\n:::\n\n\nLets see what the most negative emojis are.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemojis_sentiments %>% arrange(senti_score, .by_group = TRUE)\n```\n:::\n\n\nIts not surprising to see some emojis up there. \nStill a lot of them are also so negatively connotated because Twitter is so heavily related to politics and elections. \nThus using all of the negative emojis does not seem like the best idea. \nTherefore we handpick a few for our list:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemojis_hateful <-  \n  c(\"‚ùå\",\"‚úÇÔ∏è\",\"üíâ\",\"üî™\",\"üî™\",\"üî´\",\"üí£\",\"üêç\",\"üê∑\",\"üêµ\",\"üí©\",\"üí©\",\"üí©\",\n    \"üíÄ\",\"üë∫\",\"üëπ\",\"üë¥\",\"üëµ\",\"üò∑\",\"üò°\",\"üò†\",\"üò§\",\"üòí\",\"üöë\",\"‚ò†Ô∏è\",\"üóë\",       \n    \"üëéÔ∏è\",\"ü§¢\", \"ü§Æ\",  \"üòñ\", \"üò£\", \"üò©\", \"üò®\", \"üòù\", \"üò≥\", \"üò¨\",\n    \"üòµ\",\"üñï\",\"ü§¶‚Äç‚ôÄÔ∏è\", \"ü§¶‚Äç\" )\n```\n:::\n\n\nLets save our `hateful_emojis_list` as a data frame in our data directory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhateful_emoji_list <-\n  tibble(emoji = emojis_hateful)\n\nsave(hateful_emoji_list, file = \"data/hateful_emoji_list.RData\")\n```\n:::\n\n\n## Word Embeddings\n\nFor our `word_embeddings` we will be using the fastText model of pre-trained German embeddings,\nprovided by Deepset.ai. \nThe data can be found [here](https://www.deepset.ai/german-word-embeddings).\n\nLoading our Word Embeddings model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- \"data/model.bin\"\ndeepset_model <- load_model(x)\n```\n:::\n\n\nExtracting dictionary and word vectors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndeepset_dict <- get_dictionary(deepset_model)\nword_vectors <- get_word_vectors(deepset_model)\n```\n:::\n\n\nCreating tibble containing the dictionary words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nword_tibble <- tibble(word = deepset_dict)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nMerging the tibble with the word vectors.\nRenaming Columns and saving the file in our data folder.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(word_embeddings) <- c(\"word\", paste0(\"v\", sprintf(\"%03d\", 1:100)))\n#saveRDS(word_embeddings, file = \"data/word_embeddings.rds\")\n```\n:::\n\n\n# Recipes\n\n## Recipe 0\n\nAs a baseline recipe we use the following methods: Removal of german stop words, word stemming and normalization of all predictors and word embeddings. We also use `step_mutate` to use our curated profanities, sentiments, emojis and hateful emojis as predictors. To avoid running into memory issues, we apply a restriction for the amount of tokens (n = 100) using `step_tokenfilter`.\n\n### Defining recipe 0\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec0 <- recipe(c1 ~ ., data = select(d_train_id, text, c1, id)) %>%\n  update_role(id, new_role = \"id\") %>%\n  step_tokenize(text) %>%\n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>%\n  step_stem(text) %>%\n  step_tokenfilter(text, max_tokens = 1e2) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_numeric_predictors(), -starts_with(\"textfeature\"), -ends_with(\"_count\")) %>%\n  step_word_embeddings(text, embeddings = word_embeddings)\n\nrec0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor          1\n\nOperations:\n\nTokenization for text\nStop word removal for text\nStemming for text\nText filtering for text\nZero variance filter on all_predictors()\nCentering and scaling for all_numeric_predictors(), -starts_with(\"textfea...\nWord embeddings aggregated from text\n```\n:::\n:::\n\n\n### Preparing/Baking recipe 0\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec0_prep <- prep(rec0)\n\nrec0_bake <- bake(rec0_prep, new_data = NULL)\n\nkable(head(rec0_bake))\n```\n\n::: {.cell-output-display}\n| id|c1      | wordembed_text_v001| wordembed_text_v002| wordembed_text_v003| wordembed_text_v004| wordembed_text_v005| wordembed_text_v006| wordembed_text_v007| wordembed_text_v008| wordembed_text_v009| wordembed_text_v010| wordembed_text_v011| wordembed_text_v012| wordembed_text_v013| wordembed_text_v014| wordembed_text_v015| wordembed_text_v016| wordembed_text_v017| wordembed_text_v018| wordembed_text_v019| wordembed_text_v020| wordembed_text_v021| wordembed_text_v022| wordembed_text_v023| wordembed_text_v024| wordembed_text_v025| wordembed_text_v026| wordembed_text_v027| wordembed_text_v028| wordembed_text_v029| wordembed_text_v030| wordembed_text_v031| wordembed_text_v032| wordembed_text_v033| wordembed_text_v034| wordembed_text_v035| wordembed_text_v036| wordembed_text_v037| wordembed_text_v038| wordembed_text_v039| wordembed_text_v040| wordembed_text_v041| wordembed_text_v042| wordembed_text_v043| wordembed_text_v044| wordembed_text_v045| wordembed_text_v046| wordembed_text_v047| wordembed_text_v048| wordembed_text_v049| wordembed_text_v050| wordembed_text_v051| wordembed_text_v052| wordembed_text_v053| wordembed_text_v054| wordembed_text_v055| wordembed_text_v056| wordembed_text_v057| wordembed_text_v058| wordembed_text_v059| wordembed_text_v060| wordembed_text_v061| wordembed_text_v062| wordembed_text_v063| wordembed_text_v064| wordembed_text_v065| wordembed_text_v066| wordembed_text_v067| wordembed_text_v068| wordembed_text_v069| wordembed_text_v070| wordembed_text_v071| wordembed_text_v072| wordembed_text_v073| wordembed_text_v074| wordembed_text_v075| wordembed_text_v076| wordembed_text_v077| wordembed_text_v078| wordembed_text_v079| wordembed_text_v080| wordembed_text_v081| wordembed_text_v082| wordembed_text_v083| wordembed_text_v084| wordembed_text_v085| wordembed_text_v086| wordembed_text_v087| wordembed_text_v088| wordembed_text_v089| wordembed_text_v090| wordembed_text_v091| wordembed_text_v092| wordembed_text_v093| wordembed_text_v094| wordembed_text_v095| wordembed_text_v096| wordembed_text_v097| wordembed_text_v098| wordembed_text_v099| wordembed_text_v100|\n|--:|:-------|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|\n|  1|OTHER   |           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|           0.0000000|\n|  2|OTHER   |           0.1705173|          -0.6616511|          -0.2360490|           0.0683710|           1.0280830|          -0.3251859|           1.1333976|           0.8252960|          -0.5378390|           0.7560824|           1.5334624|          -0.5184664|          -0.9154094|           0.2952183|          -0.4663130|           0.1511446|          -0.5145182|          -0.1343736|           1.5142352|           0.0327072|           0.2083657|          -0.1150252|          -0.5707928|          -0.6057915|           0.2870054|           0.4496582|           0.5591611|           0.3997981|          -0.6996027|          -0.3608334|           1.1444200|           0.9068156|           1.4755962|           1.2194519|          -0.1052418|          -0.3718971|          -0.3845041|          -0.1559392|          -0.2755755|           0.6017619|          -0.2960374|           0.8520160|           0.3555014|           0.4958276|          -0.1173945|          -0.6466310|           0.3987075|           1.5161505|           0.1214860|           0.8627090|           0.4015738|          -0.3134273|          -1.1028645|           0.1975833|          -0.1803899|          -0.5832011|           0.5468701|          -0.0843101|           0.3153538|           0.3323961|           0.7965311|          -0.4495128|          -0.2556949|          -0.1417254|          -0.1077577|           0.1951436|          -0.2196960|           0.3355661|           0.0285472|           0.4606910|          -0.7571096|          -0.2227986|          -0.1647410|           0.8453106|           0.3920090|           0.6915095|          -1.1467145|          -0.0711268|          -0.2175048|          -0.2945035|           0.0655972|          -1.0872237|          -1.0964014|          -0.2006673|          -0.2568882|          -0.2473020|           0.1505071|          -0.8183470|          -0.8928060|           0.7143283|          -1.0193183|          -0.2238264|           1.2521546|          -0.7009847|          -0.1344296|          -0.3469929|           0.5592367|           0.3936044|          -0.5203777|           1.2654962|\n|  3|OTHER   |          -0.0718034|          -0.3881005|          -0.4741701|           0.2952131|          -0.0342643|          -0.2855173|          -0.1299734|          -0.1340009|           0.0297917|          -0.4428235|           0.3643284|          -0.2519719|          -0.4176378|           0.3386248|          -0.1512924|           0.0771755|          -0.3289341|          -0.0346081|           0.4710117|           0.2872021|          -0.2911000|          -0.3147324|          -0.5099679|           0.0166199|          -0.1726374|           0.3924557|           0.0535253|           0.0159811|          -0.3629214|          -0.0169632|           0.2446892|           0.4081479|           0.5703922|           0.3306485|           0.1378234|          -0.2761500|           0.1553301|          -0.1620139|           0.0126189|           0.0084814|          -0.2287629|           0.1391533|           0.1155296|          -0.0309295|          -0.2926523|           0.2060308|           0.0352833|           0.5536110|          -0.2343113|          -0.3766860|           0.2580437|          -0.0096653|          -0.4304293|          -0.2929336|           0.2827652|           0.1292237|          -0.0471907|          -0.1353068|          -0.0343960|          -0.0300349|           0.1915074|          -0.3006558|           0.0099447|          -0.2389557|          -0.2115490|          -0.2505156|           0.6027758|          -0.2480469|           0.3295957|           0.4221208|          -0.3628489|          -0.2900164|          -0.2980791|          -0.1701689|           0.1448324|           0.0423642|           0.1992183|           0.0712101|           0.1250661|          -0.3406907|           0.5240356|          -0.1199778|          -0.0348803|          -0.1072635|           0.0460104|           0.1025959|          -0.0416946|           0.0558642|          -0.2542820|           0.2209433|          -0.8476056|          -0.2052944|           0.3307184|          -0.1258550|           0.4011300|          -0.3439502|           0.2765861|          -0.0107792|          -0.1391599|           0.1929566|\n|  4|OTHER   |          -0.0085111|          -1.0069564|           0.1179828|          -0.0490338|           0.7585172|           0.0059626|           0.9929584|          -0.1143899|           0.1310500|           0.3445828|           1.2205725|          -0.5736997|          -1.2440808|           0.4689915|          -0.5655359|          -0.0647425|          -1.2400839|          -0.7541629|           0.3775607|          -0.0259397|           0.0323955|          -0.1318957|          -0.9763491|          -0.2599326|           0.1534037|           0.3909822|           0.3149353|           0.2164177|          -0.5465413|           0.3563783|           1.1062586|           1.3083856|           1.1106297|           1.5626093|           0.1636594|          -0.1855704|          -0.2014855|          -0.2104303|          -0.5878297|           0.9460172|           0.0000223|           0.5493760|           0.0070982|           0.0955857|           0.1869212|          -0.1218467|          -0.2646002|           0.8518468|           0.3200661|           0.8161022|          -0.4702423|          -0.4750244|          -0.8615152|          -0.3617544|           0.0367742|           0.0858123|           0.1626049|           0.0560466|           0.0924531|           0.1422715|           1.1189179|           0.2748392|           0.3107248|           0.6380534|          -0.5373157|           1.0221311|          -0.3579230|          -0.3037830|          -0.0822524|           0.3742430|           0.0347097|          -0.2821895|          -0.0191183|           0.5836134|           0.0499149|           0.8118696|          -0.8640990|          -0.4247665|          -0.2225603|          -0.8291553|           0.0581045|          -1.3873089|          -0.1723469|          -0.5633489|           0.4711508|          -0.1796506|           0.2786529|          -0.0451845|           0.0024170|           0.3011425|          -1.0393563|          -0.6167941|           0.2557316|          -0.7241205|           0.1860594|          -0.3858612|           1.0671619|           0.6950689|          -0.1327525|           0.7674235|\n|  5|OFFENSE |           0.1659442|          -0.1212116|           0.4297200|           0.1671278|           0.6733848|           0.3630850|           0.5895519|          -0.0315576|          -0.4714897|           0.6551241|          -0.2397894|          -0.1497419|          -0.3205699|          -0.1922833|          -0.2796791|          -0.1560503|          -0.5279630|          -0.0088302|           0.7276223|          -0.4894628|           0.0379584|          -0.2840639|           0.2365941|           0.1851194|           0.0751279|           0.6377639|           0.5987163|          -0.0640943|           0.2221429|           0.8524855|           0.5900680|           0.4170776|           0.3209653|           1.2468619|          -0.2500095|          -0.4789208|           0.8301271|          -0.2933840|          -0.4860361|           0.7855251|          -0.5412911|           0.0742704|          -0.2882261|          -0.4005688|          -0.2105889|           0.3432980|          -0.4213736|          -0.2314123|           0.3779839|           0.6083224|          -0.3759371|          -0.0844916|          -0.7473246|          -0.0249420|          -0.7806323|           0.4628508|          -0.1922061|          -0.0972560|           0.2900344|           0.1642629|           0.8632259|           0.4814199|           0.7437910|          -0.0854538|          -0.1830013|           0.4817045|          -0.2084624|          -0.2618224|           0.7382015|           1.0156894|           0.1109777|          -0.1784804|           0.0953212|           0.8036438|           0.3048914|          -0.1663137|          -0.5419402|           0.3779555|          -0.3867393|          -0.4770407|          -0.1481549|           0.2049418|           0.4554483|          -0.5357144|           0.4380139|           0.0237074|           0.3695869|          -0.1117402|           0.4347180|          -0.0466841|          -0.7723520|           0.4393246|          -0.1726162|          -0.0290350|           0.0706238|          -0.5646375|          -0.3045196|           0.2997910|           0.0993480|           0.5128943|\n|  6|OTHER   |           0.0723996|           0.0396071|          -0.0396552|          -0.0416255|           0.5001826|           0.1102509|           0.6520016|           0.4309873|           0.0069968|           0.0520846|           0.7101114|          -0.3311372|          -0.8754475|           0.0796991|          -0.5722652|           0.6056564|          -0.2120079|           0.1687961|           1.0053588|          -0.1777948|           0.1350512|          -0.1566600|          -0.3841559|          -0.3876189|          -0.0413104|           0.3462263|           0.3196893|           0.1564686|          -0.6565381|          -0.9370736|           0.7723243|           0.7047828|           0.7672068|           0.8356602|          -0.1170150|          -0.3570145|          -0.3462146|          -0.2935857|           0.0465142|           0.2379300|          -0.8015132|           0.6603205|           0.4118967|           0.4915452|          -0.1915982|          -0.6925082|           0.2653126|           0.9218429|           0.3628542|           0.4343772|           0.4023016|          -0.3069780|          -0.7482602|           0.3383429|          -0.3373392|          -0.4506798|          -0.0573340|          -0.1377018|           0.4374293|           0.2312903|           0.4955125|          -0.4246442|          -0.2866590|          -0.2087703|           0.0167326|          -0.0989605|           0.0833743|           0.0372744|           0.5159305|           0.2718146|          -0.7249280|          -0.0545967|          -0.4269630|           0.6871249|           0.0688320|           0.3325172|          -0.3781756|           0.3928554|           0.0338916|           0.2398159|           0.4406608|          -0.5541840|          -0.8181742|          -0.7101893|          -0.2899545|          -0.0733856|           0.2115408|          -0.4931091|          -0.6207303|           0.6299949|          -0.7597253|           0.0890892|           1.0419257|          -0.2135392|           0.1020364|          -0.0196290|           0.2177937|           0.2294836|          -0.6603450|           0.4823917|\n:::\n:::\n\n\n## Recipe 1\n\nRecipe 1 applies all steps from recipe 0 and replaces `step_word_embeddings` with `step_tf`, which converts a token variable into multiple variables containing the token counts.\n\n### Defining recipe 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec1 <- recipe(c1 ~ ., data = select(d_train_id, text, c1, id)) %>%\n  update_role(id, new_role = \"id\") %>%\n  step_tokenize(text) %>%\n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>%\n  step_stem(text) %>%\n  step_tokenfilter(text, max_tokens = 1e2) %>%\n  step_tf(text) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_numeric_predictors(), -starts_with(\"textfeature\"), -ends_with(\"_count\"))\n  \n\nrec1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor          1\n\nOperations:\n\nTokenization for text\nStop word removal for text\nStemming for text\nText filtering for text\nTerm frequency with text\nZero variance filter on all_predictors()\nCentering and scaling for all_numeric_predictors(), -starts_with(\"textfea...\n```\n:::\n:::\n\n\n### Preparing/Baking recipe 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec1_prep <- prep(rec1)\n\nrec1_bake <- bake(rec1_prep, new_data = NULL)\n\nkable(head(rec1_bake))\n```\n\n::: {.cell-output-display}\n| id|c1      | tf_text__macmik|  tf_text_2| tf_text_ab| tf_text_afd| tf_text_amp| tf_text_anna_iina| tf_text_athinamala| tf_text_beim| tf_text_besser| tf_text_bild| tf_text_cdu| tf_text_charlie_silv|  tf_text_d| tf_text_daf√ºr| tf_text_dank| tf_text_dass| tf_text_deutsch| tf_text_deutschen| tf_text_deutschland| tf_text_dumm| tf_text_eigentlich| tf_text_einfach| tf_text_ellibisathid| tf_text_endlich| tf_text_ennof_| tf_text_erst| tf_text_eu| tf_text_europa| tf_text_fdp| tf_text_feldenfrizz| tf_text_focusonlin| tf_text_frage| tf_text_frau| tf_text_ganz| tf_text_geht| tf_text_gerad| tf_text_gibt| tf_text_gr√ºnen| tf_text_gt| tf_text_gut| tf_text_h√§tte| tf_text_heut| tf_text_immer| tf_text_info2099| tf_text_islam| tf_text_israel| tf_text_ja| tf_text_jahr| tf_text_kommt| tf_text_krippmari| tf_text_land| tf_text_lassen| tf_text_lbr| tf_text_leben| tf_text_lifetrend| tf_text_link| tf_text_macht| tf_text_machtjanix23| tf_text_mal| tf_text_md_franz| tf_text_mehr| tf_text_menschen| tf_text_merkel| tf_text_miriamozen| tf_text_moslem| tf_text_m√ºssen| tf_text_nancypeggymandi| tf_text_nasanas| tf_text_nie| tf_text_noherrman| tf_text_norbinator2403| tf_text_petpanther0| tf_text_politik| tf_text_recht| tf_text_richtig| tf_text_schmiddiemaik| tf_text_schon| tf_text_schulz| tf_text_seit| tf_text_sollten| tf_text_spd| tf_text_tagesschau| tf_text_thomasgbau| tf_text_troll_putin| tf_text_trump| tf_text_tun| tf_text_t√ºrken|  tf_text_u| tf_text_unser| tf_text_viel| tf_text_volk| tf_text_w√§re| tf_text_warum| tf_text_welt| tf_text_wer| tf_text_willjrosenblatt| tf_text_wissen| tf_text_wohl| tf_text_wurd| tf_text_zeit|\n|--:|:-------|---------------:|----------:|----------:|-----------:|-----------:|-----------------:|------------------:|------------:|--------------:|------------:|-----------:|--------------------:|----------:|-------------:|------------:|------------:|---------------:|-----------------:|-------------------:|------------:|------------------:|---------------:|--------------------:|---------------:|--------------:|------------:|----------:|--------------:|-----------:|-------------------:|------------------:|-------------:|------------:|------------:|------------:|-------------:|------------:|--------------:|----------:|-----------:|-------------:|------------:|-------------:|----------------:|-------------:|--------------:|----------:|------------:|-------------:|-----------------:|------------:|--------------:|-----------:|-------------:|-----------------:|------------:|-------------:|--------------------:|-----------:|----------------:|------------:|----------------:|--------------:|------------------:|--------------:|--------------:|-----------------------:|---------------:|-----------:|-----------------:|----------------------:|-------------------:|---------------:|-------------:|---------------:|---------------------:|-------------:|--------------:|------------:|---------------:|-----------:|------------------:|------------------:|-------------------:|-------------:|-----------:|--------------:|----------:|-------------:|------------:|------------:|------------:|-------------:|------------:|-----------:|-----------------------:|--------------:|------------:|------------:|------------:|\n|  1|OTHER   |      -0.1525923| -0.1135821| -0.1241518|  -0.1831511|  -0.1358281|        -0.1198975|         -0.1525923|    -0.111939|     -0.1185582|   -0.1159489|  -0.1291369|           -0.1470207| -0.1424606|    -0.1192149|   -0.1376476|   -0.2212892|      -0.1818633|        -0.1811678|          -0.2296203|   -0.1174321|         -0.1121478|      -0.1423886|           -0.1546345|      -0.1164736|     -0.1173015|   -0.1359446| -0.1258415|     -0.1252989|  -0.1018414|          -0.1519061|         -0.1110214|    -0.1103148|   -0.1047442|   -0.1332193|   -0.1739261|    -0.1274247|   -0.1935711|     -0.1257982| -0.0936774|  -0.1491571|    -0.1092236|   -0.1935068|    -0.1933294|       -0.1599646|    -0.1182912|     -0.1048223| -0.2032149|   -0.1148201|    -0.1183531|        -0.1664105|   -0.1697051|     -0.1234393|  -0.3975076|    -0.1087085|        -0.1441613|   -0.1183531|    -0.1452667|           -0.1455975|  -0.2076412|       -0.1553098|    -0.216943|       -0.1479409|     -0.2551084|         -0.1215991|     -0.1035222|     -0.1499246|              -0.1181729|      -0.1470207|  -0.1065601|        -0.1573199|             -0.1232786|          -0.1573199|      -0.1755382|     -0.143107|      -0.1190847|            -0.1427119|    -0.2375755|     -0.1157319|   -0.1491959|      -0.1137532|  -0.1576631|         -0.1110781|         -0.1519061|          -0.1232786|     -0.115067|  -0.1207959|     -0.1215991| -0.1714487|    -0.1808551|   -0.1166118|   -0.1285038|   -0.1341874|    -0.1244491|   -0.1727265|  -0.1833771|              -0.1427119|     -0.1049151|   -0.1233318|   -0.1368046|   -0.1188249|\n|  2|OTHER   |      -0.1525923| -0.1135821| -0.1241518|  -0.1831511|  -0.1358281|        -0.1198975|         -0.1525923|    -0.111939|     -0.1185582|   -0.1159489|  -0.1291369|           -0.1470207| -0.1424606|    -0.1192149|   -0.1376476|    3.9773384|      -0.1818633|        -0.1811678|          -0.2296203|   -0.1174321|         -0.1121478|      -0.1423886|           -0.1546345|      -0.1164736|     -0.1173015|   -0.1359446| -0.1258415|     -0.1252989|  -0.1018414|          -0.1519061|         -0.1110214|    -0.1103148|   -0.1047442|   -0.1332193|   -0.1739261|    -0.1274247|   -0.1935711|     -0.1257982| -0.0936774|  -0.1491571|    -0.1092236|   -0.1935068|    -0.1933294|       -0.1599646|    -0.1182912|     -0.1048223|  4.4875846|   -0.1148201|    -0.1183531|        -0.1664105|   -0.1697051|     -0.1234393|  -0.3975076|    -0.1087085|        -0.1441613|   -0.1183531|    -0.1452667|           -0.1455975|  -0.2076412|       -0.1553098|    -0.216943|       -0.1479409|     -0.2551084|         -0.1215991|     -0.1035222|     -0.1499246|              -0.1181729|      -0.1470207|  -0.1065601|        -0.1573199|             -0.1232786|          -0.1573199|      -0.1755382|      6.200457|      -0.1190847|            -0.1427119|    -0.2375755|     -0.1157319|   -0.1491959|      -0.1137532|  -0.1576631|         -0.1110781|         -0.1519061|          -0.1232786|     -0.115067|  -0.1207959|     -0.1215991| -0.1714487|    -0.1808551|   -0.1166118|   -0.1285038|   -0.1341874|    -0.1244491|   -0.1727265|  -0.1833771|              -0.1427119|     -0.1049151|   -0.1233318|   -0.1368046|   -0.1188249|\n|  3|OTHER   |      -0.1525923| -0.1135821| -0.1241518|  -0.1831511|  -0.1358281|        -0.1198975|         -0.1525923|    -0.111939|     -0.1185582|   -0.1159489|  -0.1291369|           -0.1470207| -0.1424606|    -0.1192149|   -0.1376476|   -0.2212892|      -0.1818633|        -0.1811678|          -0.2296203|   -0.1174321|         -0.1121478|      -0.1423886|           -0.1546345|      -0.1164736|     -0.1173015|   -0.1359446| -0.1258415|     -0.1252989|  -0.1018414|          -0.1519061|         -0.1110214|    -0.1103148|   -0.1047442|   -0.1332193|   -0.1739261|    -0.1274247|   -0.1935711|     -0.1257982| -0.0936774|  -0.1491571|    -0.1092236|   -0.1935068|    -0.1933294|       -0.1599646|    -0.1182912|     -0.1048223| -0.2032149|   -0.1148201|    -0.1183531|        -0.1664105|   -0.1697051|     -0.1234393|  -0.3975076|    -0.1087085|        -0.1441613|   -0.1183531|    -0.1452667|           -0.1455975|  -0.2076412|       -0.1553098|    -0.216943|       -0.1479409|     -0.2551084|         -0.1215991|     -0.1035222|     -0.1499246|              -0.1181729|      -0.1470207|  -0.1065601|        -0.1573199|             -0.1232786|          -0.1573199|      -0.1755382|     -0.143107|      -0.1190847|            -0.1427119|    -0.2375755|     -0.1157319|   -0.1491959|      -0.1137532|  -0.1576631|         -0.1110781|         -0.1519061|          -0.1232786|     -0.115067|  -0.1207959|     -0.1215991| -0.1714487|    -0.1808551|   -0.1166118|   -0.1285038|   -0.1341874|    -0.1244491|    5.3031406|  -0.1833771|              -0.1427119|     -0.1049151|   -0.1233318|   -0.1368046|   -0.1188249|\n|  4|OTHER   |      -0.1525923| -0.1135821| -0.1241518|  -0.1831511|  -0.1358281|        -0.1198975|         -0.1525923|    -0.111939|     -0.1185582|   -0.1159489|  -0.1291369|           -0.1470207| -0.1424606|    -0.1192149|   -0.1376476|   -0.2212892|      -0.1818633|        -0.1811678|          -0.2296203|   -0.1174321|         -0.1121478|      -0.1423886|           -0.1546345|      -0.1164736|     -0.1173015|    6.6735214| -0.1258415|     -0.1252989|  -0.1018414|          -0.1519061|         -0.1110214|    -0.1103148|   -0.1047442|   -0.1332193|   -0.1739261|    -0.1274247|   -0.1935711|     -0.1257982| -0.0936774|  -0.1491571|    -0.1092236|   -0.1935068|    -0.1933294|       -0.1599646|    -0.1182912|     -0.1048223| -0.2032149|   -0.1148201|    -0.1183531|        -0.1664105|   -0.1697051|     -0.1234393|  -0.3975076|    -0.1087085|        -0.1441613|   -0.1183531|    -0.1452667|           -0.1455975|  -0.2076412|       -0.1553098|    -0.216943|       -0.1479409|     -0.2551084|         -0.1215991|     -0.1035222|     -0.1499246|              -0.1181729|      -0.1470207|  -0.1065601|        -0.1573199|             -0.1232786|          -0.1573199|       5.1212746|      6.200457|      -0.1190847|            -0.1427119|    -0.2375755|     -0.1157319|   -0.1491959|      -0.1137532|  -0.1576631|         -0.1110781|         -0.1519061|          -0.1232786|     -0.115067|  -0.1207959|     -0.1215991| -0.1714487|    -0.1808551|   -0.1166118|   -0.1285038|   -0.1341874|    -0.1244491|   -0.1727265|  -0.1833771|              -0.1427119|     -0.1049151|   -0.1233318|   -0.1368046|   -0.1188249|\n|  5|OFFENSE |      -0.1525923| -0.1135821| -0.1241518|  -0.1831511|  -0.1358281|        -0.1198975|         -0.1525923|    -0.111939|     -0.1185582|   -0.1159489|  -0.1291369|           -0.1470207| -0.1424606|    -0.1192149|   -0.1376476|   -0.2212892|      -0.1818633|        -0.1811678|          -0.2296203|   -0.1174321|         -0.1121478|      -0.1423886|           -0.1546345|      -0.1164736|     -0.1173015|   -0.1359446| -0.1258415|     -0.1252989|  -0.1018414|          -0.1519061|         -0.1110214|    -0.1103148|   -0.1047442|   -0.1332193|   -0.1739261|    -0.1274247|   -0.1935711|     -0.1257982| -0.0936774|  -0.1491571|    -0.1092236|   -0.1935068|    -0.1933294|       -0.1599646|    -0.1182912|     -0.1048223| -0.2032149|   -0.1148201|    -0.1183531|        -0.1664105|   -0.1697051|     -0.1234393|  -0.3975076|    -0.1087085|        -0.1441613|   -0.1183531|    -0.1452667|           -0.1455975|  -0.2076412|       -0.1553098|    -0.216943|       -0.1479409|     -0.2551084|         -0.1215991|     -0.1035222|     -0.1499246|              -0.1181729|      -0.1470207|  -0.1065601|        -0.1573199|             -0.1232786|          -0.1573199|      -0.1755382|     -0.143107|      -0.1190847|            -0.1427119|    -0.2375755|     -0.1157319|   -0.1491959|      -0.1137532|   5.5238815|         -0.1110781|         -0.1519061|          -0.1232786|     -0.115067|  -0.1207959|     -0.1215991| -0.1714487|    -0.1808551|   -0.1166118|   -0.1285038|   -0.1341874|    -0.1244491|   -0.1727265|  -0.1833771|              -0.1427119|     -0.1049151|   -0.1233318|   -0.1368046|   -0.1188249|\n|  6|OTHER   |      -0.1525923| -0.1135821| -0.1241518|  -0.1831511|  -0.1358281|        -0.1198975|         -0.1525923|    -0.111939|     -0.1185582|   -0.1159489|  -0.1291369|           -0.1470207| -0.1424606|    -0.1192149|   -0.1376476|   -0.2212892|      -0.1818633|        -0.1811678|          -0.2296203|   -0.1174321|         -0.1121478|      -0.1423886|           -0.1546345|      -0.1164736|     -0.1173015|   -0.1359446| -0.1258415|     -0.1252989|  -0.1018414|          -0.1519061|         -0.1110214|    -0.1103148|   -0.1047442|   -0.1332193|    5.5955842|    -0.1274247|   -0.1935711|     -0.1257982| -0.0936774|  -0.1491571|    -0.1092236|   -0.1935068|    -0.1933294|       -0.1599646|    -0.1182912|     -0.1048223|  4.4875846|   -0.1148201|    -0.1183531|        -0.1664105|   -0.1697051|     -0.1234393|  -0.3975076|    -0.1087085|        -0.1441613|   -0.1183531|    -0.1452667|           -0.1455975|  -0.2076412|       -0.1553098|    -0.216943|       -0.1479409|     -0.2551084|         -0.1215991|     -0.1035222|     -0.1499246|              -0.1181729|      -0.1470207|  -0.1065601|        -0.1573199|             -0.1232786|          -0.1573199|      -0.1755382|     -0.143107|      -0.1190847|            -0.1427119|    -0.2375755|     -0.1157319|   -0.1491959|      -0.1137532|  -0.1576631|         -0.1110781|         -0.1519061|          -0.1232786|     -0.115067|  -0.1207959|     -0.1215991| -0.1714487|    -0.1808551|   -0.1166118|   -0.1285038|   -0.1341874|    -0.1244491|   -0.1727265|  -0.1833771|              -0.1427119|     -0.1049151|   -0.1233318|   -0.1368046|   -0.1188249|\n:::\n:::\n\n\n## Recipe 2\n\nIn this recipe we change `step_tf` to `step_tfidf`, which results in an inverse Document Frequency of our tokens.\n\n### Defining recipe 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec2 <- recipe(c1 ~ ., data = select(d_train_id, text, c1, id)) %>%\n  update_role(id, new_role = \"id\") %>%\n  step_tokenize(text) %>%\n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>%\n  step_stem(text) %>%\n  step_tokenfilter(text, max_tokens = 1e2) %>%\n  step_tfidf(text) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_numeric_predictors(), -starts_with(\"textfeature\"), -ends_with(\"_count\"))\n  \n\nrec2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor          1\n\nOperations:\n\nTokenization for text\nStop word removal for text\nStemming for text\nText filtering for text\nTerm frequency-inverse document frequency with text\nZero variance filter on all_predictors()\nCentering and scaling for all_numeric_predictors(), -starts_with(\"textfea...\n```\n:::\n:::\n\n\n### Preparing/Baking recipe 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec2_prep <- prep(rec2)\n\nrec2_bake <- bake(rec2_prep, new_data = NULL)\n\nkable(head(rec2_bake))\n```\n\n::: {.cell-output-display}\n| id|c1      | tfidf_text__macmik| tfidf_text_2| tfidf_text_ab| tfidf_text_afd| tfidf_text_amp| tfidf_text_anna_iina| tfidf_text_athinamala| tfidf_text_beim| tfidf_text_besser| tfidf_text_bild| tfidf_text_cdu| tfidf_text_charlie_silv| tfidf_text_d| tfidf_text_daf√ºr| tfidf_text_dank| tfidf_text_dass| tfidf_text_deutsch| tfidf_text_deutschen| tfidf_text_deutschland| tfidf_text_dumm| tfidf_text_eigentlich| tfidf_text_einfach| tfidf_text_ellibisathid| tfidf_text_endlich| tfidf_text_ennof_| tfidf_text_erst| tfidf_text_eu| tfidf_text_europa| tfidf_text_fdp| tfidf_text_feldenfrizz| tfidf_text_focusonlin| tfidf_text_frage| tfidf_text_frau| tfidf_text_ganz| tfidf_text_geht| tfidf_text_gerad| tfidf_text_gibt| tfidf_text_gr√ºnen| tfidf_text_gt| tfidf_text_gut| tfidf_text_h√§tte| tfidf_text_heut| tfidf_text_immer| tfidf_text_info2099| tfidf_text_islam| tfidf_text_israel| tfidf_text_ja| tfidf_text_jahr| tfidf_text_kommt| tfidf_text_krippmari| tfidf_text_land| tfidf_text_lassen| tfidf_text_lbr| tfidf_text_leben| tfidf_text_lifetrend| tfidf_text_link| tfidf_text_macht| tfidf_text_machtjanix23| tfidf_text_mal| tfidf_text_md_franz| tfidf_text_mehr| tfidf_text_menschen| tfidf_text_merkel| tfidf_text_miriamozen| tfidf_text_moslem| tfidf_text_m√ºssen| tfidf_text_nancypeggymandi| tfidf_text_nasanas| tfidf_text_nie| tfidf_text_noherrman| tfidf_text_norbinator2403| tfidf_text_petpanther0| tfidf_text_politik| tfidf_text_recht| tfidf_text_richtig| tfidf_text_schmiddiemaik| tfidf_text_schon| tfidf_text_schulz| tfidf_text_seit| tfidf_text_sollten| tfidf_text_spd| tfidf_text_tagesschau| tfidf_text_thomasgbau| tfidf_text_troll_putin| tfidf_text_trump| tfidf_text_tun| tfidf_text_t√ºrken| tfidf_text_u| tfidf_text_unser| tfidf_text_viel| tfidf_text_volk| tfidf_text_w√§re| tfidf_text_warum| tfidf_text_welt| tfidf_text_wer| tfidf_text_willjrosenblatt| tfidf_text_wissen| tfidf_text_wohl| tfidf_text_wurd| tfidf_text_zeit|\n|--:|:-------|------------------:|------------:|-------------:|--------------:|--------------:|--------------------:|---------------------:|---------------:|-----------------:|---------------:|--------------:|-----------------------:|------------:|----------------:|---------------:|---------------:|------------------:|--------------------:|----------------------:|---------------:|---------------------:|------------------:|-----------------------:|------------------:|-----------------:|---------------:|-------------:|-----------------:|--------------:|----------------------:|---------------------:|----------------:|---------------:|---------------:|---------------:|----------------:|---------------:|-----------------:|-------------:|--------------:|----------------:|---------------:|----------------:|-------------------:|----------------:|-----------------:|-------------:|---------------:|----------------:|--------------------:|---------------:|-----------------:|--------------:|----------------:|--------------------:|---------------:|----------------:|-----------------------:|--------------:|-------------------:|---------------:|-------------------:|-----------------:|---------------------:|-----------------:|-----------------:|--------------------------:|------------------:|--------------:|--------------------:|-------------------------:|----------------------:|------------------:|----------------:|------------------:|------------------------:|----------------:|-----------------:|---------------:|------------------:|--------------:|---------------------:|---------------------:|----------------------:|----------------:|--------------:|-----------------:|------------:|----------------:|---------------:|---------------:|---------------:|----------------:|---------------:|--------------:|--------------------------:|-----------------:|---------------:|---------------:|---------------:|\n|  1|OTHER   |         -0.1360347|   -0.0996499|    -0.1032711|     -0.1592451|     -0.1213167|            -0.102885|            -0.1360347|      -0.0906374|        -0.0998114|      -0.1001756|     -0.1057297|              -0.1436627|   -0.1343985|       -0.0965294|      -0.1194715|      -0.1919072|          -0.156776|           -0.1510832|             -0.2000219|      -0.1041115|            -0.0933891|         -0.1219992|               -0.111245|         -0.1005732|        -0.1170189|      -0.1139481|    -0.1151372|        -0.1073693|     -0.0916503|             -0.1460452|            -0.0971611|       -0.0941145|      -0.0912738|       -0.114278|      -0.1489552|       -0.1094171|      -0.1653518|         -0.107322|     -0.090492|     -0.1221851|       -0.0958297|      -0.1672072|        -0.166724|          -0.0785723|       -0.1057581|        -0.0888228|    -0.1730153|      -0.0993418|       -0.0993865|            -0.142428|      -0.1462041|        -0.1071178|     -0.4070735|       -0.0884154|           -0.1428669|      -0.1018528|       -0.1229545|              -0.1102224|     -0.1740563|          -0.1115021|      -0.1842209|          -0.1233943|        -0.2234377|            -0.1036738|        -0.0924792|        -0.1307096|                 -0.1150833|         -0.1103349|     -0.0892396|           -0.0944897|                -0.0641121|             -0.0841552|         -0.1518153|       -0.1190563|         -0.0997825|                -0.141485|       -0.1996958|        -0.1007793|      -0.1239379|         -0.0978532|     -0.1317353|            -0.0990086|            -0.1409355|             -0.1188606|       -0.1016846|     -0.1045154|        -0.1048411|   -0.1538774|       -0.1553256|      -0.0982374|      -0.1096479|      -0.1128127|       -0.1046822|      -0.1496353|     -0.1584449|                  -0.141485|         -0.091336|      -0.1052044|      -0.1138908|      -0.1013744|\n|  2|OTHER   |         -0.1360347|   -0.0996499|    -0.1032711|     -0.1592451|     -0.1213167|            -0.102885|            -0.1360347|      -0.0906374|        -0.0998114|      -0.1001756|     -0.1057297|              -0.1436627|   -0.1343985|       -0.0965294|      -0.1194715|       3.4468883|          -0.156776|           -0.1510832|             -0.2000219|      -0.1041115|            -0.0933891|         -0.1219992|               -0.111245|         -0.1005732|        -0.1170189|      -0.1139481|    -0.1151372|        -0.1073693|     -0.0916503|             -0.1460452|            -0.0971611|       -0.0941145|      -0.0912738|       -0.114278|      -0.1489552|       -0.1094171|      -0.1653518|         -0.107322|     -0.090492|     -0.1221851|       -0.0958297|      -0.1672072|        -0.166724|          -0.0785723|       -0.1057581|        -0.0888228|     3.4000247|      -0.0993418|       -0.0993865|            -0.142428|      -0.1462041|        -0.1071178|     -0.4070735|       -0.0884154|           -0.1428669|      -0.1018528|       -0.1229545|              -0.1102224|     -0.1740563|          -0.1115021|      -0.1842209|          -0.1233943|        -0.2234377|            -0.1036738|        -0.0924792|        -0.1307096|                 -0.1150833|         -0.1103349|     -0.0892396|           -0.0944897|                -0.0641121|             -0.0841552|         -0.1518153|        4.7112064|         -0.0997825|                -0.141485|       -0.1996958|        -0.1007793|      -0.1239379|         -0.0978532|     -0.1317353|            -0.0990086|            -0.1409355|             -0.1188606|       -0.1016846|     -0.1045154|        -0.1048411|   -0.1538774|       -0.1553256|      -0.0982374|      -0.1096479|      -0.1128127|       -0.1046822|      -0.1496353|     -0.1584449|                  -0.141485|         -0.091336|      -0.1052044|      -0.1138908|      -0.1013744|\n|  3|OTHER   |         -0.1360347|   -0.0996499|    -0.1032711|     -0.1592451|     -0.1213167|            -0.102885|            -0.1360347|      -0.0906374|        -0.0998114|      -0.1001756|     -0.1057297|              -0.1436627|   -0.1343985|       -0.0965294|      -0.1194715|      -0.1919072|          -0.156776|           -0.1510832|             -0.2000219|      -0.1041115|            -0.0933891|         -0.1219992|               -0.111245|         -0.1005732|        -0.1170189|      -0.1139481|    -0.1151372|        -0.1073693|     -0.0916503|             -0.1460452|            -0.0971611|       -0.0941145|      -0.0912738|       -0.114278|      -0.1489552|       -0.1094171|      -0.1653518|         -0.107322|     -0.090492|     -0.1221851|       -0.0958297|      -0.1672072|        -0.166724|          -0.0785723|       -0.1057581|        -0.0888228|    -0.1730153|      -0.0993418|       -0.0993865|            -0.142428|      -0.1462041|        -0.1071178|     -0.4070735|       -0.0884154|           -0.1428669|      -0.1018528|       -0.1229545|              -0.1102224|     -0.1740563|          -0.1115021|      -0.1842209|          -0.1233943|        -0.2234377|            -0.1036738|        -0.0924792|        -0.1307096|                 -0.1150833|         -0.1103349|     -0.0892396|           -0.0944897|                -0.0641121|             -0.0841552|         -0.1518153|       -0.1190563|         -0.0997825|                -0.141485|       -0.1996958|        -0.1007793|      -0.1239379|         -0.0978532|     -0.1317353|            -0.0990086|            -0.1409355|             -0.1188606|       -0.1016846|     -0.1045154|        -0.1048411|   -0.1538774|       -0.1553256|      -0.0982374|      -0.1096479|      -0.1128127|       -0.1046822|      10.9991033|     -0.1584449|                  -0.141485|         -0.091336|      -0.1052044|      -0.1138908|      -0.1013744|\n|  4|OTHER   |         -0.1360347|   -0.0996499|    -0.1032711|     -0.1592451|     -0.1213167|            -0.102885|            -0.1360347|      -0.0906374|        -0.0998114|      -0.1001756|     -0.1057297|              -0.1436627|   -0.1343985|       -0.0965294|      -0.1194715|      -0.1919072|          -0.156776|           -0.1510832|             -0.2000219|      -0.1041115|            -0.0933891|         -0.1219992|               -0.111245|         -0.1005732|        -0.1170189|       5.7532137|    -0.1151372|        -0.1073693|     -0.0916503|             -0.1460452|            -0.0971611|       -0.0941145|      -0.0912738|       -0.114278|      -0.1489552|       -0.1094171|      -0.1653518|         -0.107322|     -0.090492|     -0.1221851|       -0.0958297|      -0.1672072|        -0.166724|          -0.0785723|       -0.1057581|        -0.0888228|    -0.1730153|      -0.0993418|       -0.0993865|            -0.142428|      -0.1462041|        -0.1071178|     -0.4070735|       -0.0884154|           -0.1428669|      -0.1018528|       -0.1229545|              -0.1102224|     -0.1740563|          -0.1115021|      -0.1842209|          -0.1233943|        -0.2234377|            -0.1036738|        -0.0924792|        -0.1307096|                 -0.1150833|         -0.1103349|     -0.0892396|           -0.0944897|                -0.0641121|             -0.0841552|          4.3093516|        4.7112064|         -0.0997825|                -0.141485|       -0.1996958|        -0.1007793|      -0.1239379|         -0.0978532|     -0.1317353|            -0.0990086|            -0.1409355|             -0.1188606|       -0.1016846|     -0.1045154|        -0.1048411|   -0.1538774|       -0.1553256|      -0.0982374|      -0.1096479|      -0.1128127|       -0.1046822|      -0.1496353|     -0.1584449|                  -0.141485|         -0.091336|      -0.1052044|      -0.1138908|      -0.1013744|\n|  5|OFFENSE |         -0.1360347|   -0.0996499|    -0.1032711|     -0.1592451|     -0.1213167|            -0.102885|            -0.1360347|      -0.0906374|        -0.0998114|      -0.1001756|     -0.1057297|              -0.1436627|   -0.1343985|       -0.0965294|      -0.1194715|      -0.1919072|          -0.156776|           -0.1510832|             -0.2000219|      -0.1041115|            -0.0933891|         -0.1219992|               -0.111245|         -0.1005732|        -0.1170189|      -0.1139481|    -0.1151372|        -0.1073693|     -0.0916503|             -0.1460452|            -0.0971611|       -0.0941145|      -0.0912738|       -0.114278|      -0.1489552|       -0.1094171|      -0.1653518|         -0.107322|     -0.090492|     -0.1221851|       -0.0958297|      -0.1672072|        -0.166724|          -0.0785723|       -0.1057581|        -0.0888228|    -0.1730153|      -0.0993418|       -0.0993865|            -0.142428|      -0.1462041|        -0.1071178|     -0.4070735|       -0.0884154|           -0.1428669|      -0.1018528|       -0.1229545|              -0.1102224|     -0.1740563|          -0.1115021|      -0.1842209|          -0.1233943|        -0.2234377|            -0.1036738|        -0.0924792|        -0.1307096|                 -0.1150833|         -0.1103349|     -0.0892396|           -0.0944897|                -0.0641121|             -0.0841552|         -0.1518153|       -0.1190563|         -0.0997825|                -0.141485|       -0.1996958|        -0.1007793|      -0.1239379|         -0.0978532|     13.9000535|            -0.0990086|            -0.1409355|             -0.1188606|       -0.1016846|     -0.1045154|        -0.1048411|   -0.1538774|       -0.1553256|      -0.0982374|      -0.1096479|      -0.1128127|       -0.1046822|      -0.1496353|     -0.1584449|                  -0.141485|         -0.091336|      -0.1052044|      -0.1138908|      -0.1013744|\n|  6|OTHER   |         -0.1360347|   -0.0996499|    -0.1032711|     -0.1592451|     -0.1213167|            -0.102885|            -0.1360347|      -0.0906374|        -0.0998114|      -0.1001756|     -0.1057297|              -0.1436627|   -0.1343985|       -0.0965294|      -0.1194715|      -0.1919072|          -0.156776|           -0.1510832|             -0.2000219|      -0.1041115|            -0.0933891|         -0.1219992|               -0.111245|         -0.1005732|        -0.1170189|      -0.1139481|    -0.1151372|        -0.1073693|     -0.0916503|             -0.1460452|            -0.0971611|       -0.0941145|      -0.0912738|       -0.114278|       6.1420345|       -0.1094171|      -0.1653518|         -0.107322|     -0.090492|     -0.1221851|       -0.0958297|      -0.1672072|        -0.166724|          -0.0785723|       -0.1057581|        -0.0888228|     5.1865448|      -0.0993418|       -0.0993865|            -0.142428|      -0.1462041|        -0.1071178|     -0.4070735|       -0.0884154|           -0.1428669|      -0.1018528|       -0.1229545|              -0.1102224|     -0.1740563|          -0.1115021|      -0.1842209|          -0.1233943|        -0.2234377|            -0.1036738|        -0.0924792|        -0.1307096|                 -0.1150833|         -0.1103349|     -0.0892396|           -0.0944897|                -0.0641121|             -0.0841552|         -0.1518153|       -0.1190563|         -0.0997825|                -0.141485|       -0.1996958|        -0.1007793|      -0.1239379|         -0.0978532|     -0.1317353|            -0.0990086|            -0.1409355|             -0.1188606|       -0.1016846|     -0.1045154|        -0.1048411|   -0.1538774|       -0.1553256|      -0.0982374|      -0.1096479|      -0.1128127|       -0.1046822|      -0.1496353|     -0.1584449|                  -0.141485|         -0.091336|      -0.1052044|      -0.1138908|      -0.1013744|\n:::\n:::\n\n\n# Models\n\n## Naive Bayes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_nb <- naive_Bayes() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"naivebayes\")\n\nm_nb\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNaive Bayes Model Specification (classification)\n\nComputational engine: naivebayes \n```\n:::\n:::\n\n\n## Boost Trees - XGBoost\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel()\n  \nm_xgb <- boost_tree(trees = tune()) %>% \nset_engine(\"xgboost\", nthreads = 12) %>% \nset_mode(\"classification\")\n```\n:::\n\n\n## Lasso Model\n\n-\\> Regression model, penalized with the L1-norm (sum of the absolute coefficients).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel()\ncores <- parallel::detectCores(logical = TRUE)\n\nm_l <- logistic_reg(penalty = tune(), mixture = 1) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"glmnet\", num.threads = cores)\n\nm_l\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nEngine-Specific Arguments:\n  num.threads = cores\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n## Ridge Regression\n\n-\\> Creates a model that is penalized with the L2-norm. With that we can shrink the coefficient values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel()\ncores <- parallel::detectCores(logical = TRUE)\n\nm_rr <- logistic_reg(penalty = tune(), mixture = 0) %>%\n    set_mode(\"classification\") %>%\n    set_engine(\"glmnet\")\n\nm_rr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n## Elastic Net Model\n\n-\\> Creates a regression model that is penalized with both the L1-norm and L2-norm. More or less a combination of Lasso and Ridge.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_en <- logistic_reg(penalty = tune(), mixture = 0.5) %>%\n    set_mode(\"classification\") %>%\n    set_engine(\"glmnet\")\n\nm_en\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0.5\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n## Cross Validation\n\nWe will be using the regular 10x cross validation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(13)\ncv_folds <- vfold_cv(d_train_id, v = 10)\n```\n:::\n\n\n## Lambda Grid\n\nWe will be using a lambda grid with a total 30 levels.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda_grid <- grid_regular(penalty(), levels = 30)\n```\n:::\n\n\n# Workflows\n\n## Workflow Rec0 - Naive Bayes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r0_nb <- workflow() %>%\n  add_recipe(rec0) %>%\n  add_model(m_nb)\n```\n:::\n\n\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_r0_nb <- fit_resamples(wf_r0_nb, cv_folds)\n```\n:::\n\n\n### Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r0_nb_performance <- collect_metrics(fit_r0_nb)\n\nwf_r0_nb_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.552    10 0.00525 Preprocessor1_Model1\n2 roc_auc  binary     0.605    10 0.00608 Preprocessor1_Model1\n```\n:::\n:::\n\n\n## Workflow Rec1 - Naive Bayes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r1_nb <- workflow() %>%\n  add_recipe(rec1) %>%\n  add_model(m_nb)\n```\n:::\n\n\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_r1_nb <- fit_resamples(wf_r1_nb, cv_folds)\n```\n:::\n\n\n### Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r1_nb_performance <- collect_metrics(fit_r1_nb)\n\nwf_r1_nb_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.663    10 0.00515 Preprocessor1_Model1\n2 roc_auc  binary     0.655    10 0.00539 Preprocessor1_Model1\n```\n:::\n:::\n\n\n## Workflow Rec2 - Naive Bayes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r2_nb <- workflow() %>%\n  add_recipe(rec2) %>%\n  add_model(m_nb)\n```\n:::\n\n\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_r2_nb <- fit_resamples(wf_r2_nb, cv_folds)\n```\n:::\n\n\n### Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r2_nb_performance <- collect_metrics(fit_r2_nb)\n\nwf_r2_nb_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.664    10 0.00446 Preprocessor1_Model1\n2 roc_auc  binary     0.613    10 0.00276 Preprocessor1_Model1\n```\n:::\n:::\n\n\n## Workflow Rec0 - XGBoost\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r0_xgb <- workflow() %>%\n  add_recipe(rec0) %>%\n  add_model(m_xgb)\n```\n:::\n\n\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2246)\n\nfit_r0_xgb <- tune_grid(wf_r0_xgb, cv_folds, grid = 10, \n                        control = control_resamples(save_pred = TRUE))\n```\n:::\n\n\n### Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r0_xgb_performance <- collect_metrics(fit_r0_xgb)\n\nwf_r0_xgb_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 x 7\n   trees .metric  .estimator  mean     n std_err .config              \n   <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1    23 accuracy binary     0.667    10 0.00596 Preprocessor1_Model01\n 2    23 roc_auc  binary     0.629    10 0.00704 Preprocessor1_Model01\n 3   223 accuracy binary     0.675    10 0.00627 Preprocessor1_Model02\n 4   223 roc_auc  binary     0.620    10 0.00635 Preprocessor1_Model02\n 5   459 accuracy binary     0.677    10 0.00547 Preprocessor1_Model03\n 6   459 roc_auc  binary     0.617    10 0.00704 Preprocessor1_Model03\n 7   795 accuracy binary     0.677    10 0.00582 Preprocessor1_Model04\n 8   795 roc_auc  binary     0.617    10 0.00715 Preprocessor1_Model04\n 9   838 accuracy binary     0.675    10 0.00572 Preprocessor1_Model05\n10   838 roc_auc  binary     0.616    10 0.00715 Preprocessor1_Model05\n11  1150 accuracy binary     0.677    10 0.00564 Preprocessor1_Model06\n12  1150 roc_auc  binary     0.615    10 0.00714 Preprocessor1_Model06\n13  1239 accuracy binary     0.679    10 0.00557 Preprocessor1_Model07\n14  1239 roc_auc  binary     0.615    10 0.00719 Preprocessor1_Model07\n15  1464 accuracy binary     0.678    10 0.00532 Preprocessor1_Model08\n16  1464 roc_auc  binary     0.616    10 0.00705 Preprocessor1_Model08\n17  1620 accuracy binary     0.677    10 0.00573 Preprocessor1_Model09\n18  1620 roc_auc  binary     0.615    10 0.00711 Preprocessor1_Model09\n19  1930 accuracy binary     0.677    10 0.00559 Preprocessor1_Model10\n20  1930 roc_auc  binary     0.615    10 0.00689 Preprocessor1_Model10\n```\n:::\n:::\n\n\n## Workflow Rec1 - XGBoost\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r1_xgb <- workflow() %>%\n  add_recipe(rec1) %>%\n  add_model(m_xgb)\n```\n:::\n\n\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2246)\n\nfit_r1_xgb <- tune_grid(wf_r1_xgb, cv_folds, grid = 10, \n                        control = control_resamples(save_pred = TRUE))\n```\n:::\n\n\n### Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r1_xgb_performance <- collect_metrics(fit_r1_xgb)\n\nwf_r1_xgb_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 x 7\n   trees .metric  .estimator  mean     n std_err .config              \n   <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1    23 accuracy binary     0.693    10 0.00332 Preprocessor1_Model01\n 2    23 roc_auc  binary     0.661    10 0.00646 Preprocessor1_Model01\n 3   223 accuracy binary     0.687    10 0.00573 Preprocessor1_Model02\n 4   223 roc_auc  binary     0.648    10 0.00851 Preprocessor1_Model02\n 5   459 accuracy binary     0.683    10 0.00669 Preprocessor1_Model03\n 6   459 roc_auc  binary     0.643    10 0.00931 Preprocessor1_Model03\n 7   795 accuracy binary     0.681    10 0.00707 Preprocessor1_Model04\n 8   795 roc_auc  binary     0.639    10 0.00963 Preprocessor1_Model04\n 9   838 accuracy binary     0.681    10 0.00681 Preprocessor1_Model05\n10   838 roc_auc  binary     0.639    10 0.00973 Preprocessor1_Model05\n11  1150 accuracy binary     0.679    10 0.00646 Preprocessor1_Model06\n12  1150 roc_auc  binary     0.638    10 0.00942 Preprocessor1_Model06\n13  1239 accuracy binary     0.678    10 0.00656 Preprocessor1_Model07\n14  1239 roc_auc  binary     0.638    10 0.00981 Preprocessor1_Model07\n15  1464 accuracy binary     0.677    10 0.00719 Preprocessor1_Model08\n16  1464 roc_auc  binary     0.637    10 0.00950 Preprocessor1_Model08\n17  1620 accuracy binary     0.675    10 0.00634 Preprocessor1_Model09\n18  1620 roc_auc  binary     0.637    10 0.00954 Preprocessor1_Model09\n19  1930 accuracy binary     0.675    10 0.00594 Preprocessor1_Model10\n20  1930 roc_auc  binary     0.635    10 0.00884 Preprocessor1_Model10\n```\n:::\n:::\n\n\n## Workflow Rec2 - XGBoost\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r2_xgb <- workflow() %>%\n  add_recipe(rec2) %>%\n  add_model(m_xgb)\n```\n:::\n\n\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2246)\n\nfit_r2_xgb <- tune_grid(wf_r2_xgb, cv_folds, grid = 10, \n                        control = control_resamples(save_pred = TRUE))\n```\n:::\n\n\n### Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r2_xgb_performance <- collect_metrics(fit_r2_xgb)\n\nwf_r2_xgb_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 x 7\n   trees .metric  .estimator  mean     n std_err .config              \n   <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1    23 accuracy binary     0.695    10 0.00383 Preprocessor1_Model01\n 2    23 roc_auc  binary     0.655    10 0.00585 Preprocessor1_Model01\n 3   223 accuracy binary     0.680    10 0.00391 Preprocessor1_Model02\n 4   223 roc_auc  binary     0.644    10 0.00679 Preprocessor1_Model02\n 5   459 accuracy binary     0.672    10 0.00308 Preprocessor1_Model03\n 6   459 roc_auc  binary     0.631    10 0.00849 Preprocessor1_Model03\n 7   795 accuracy binary     0.669    10 0.00435 Preprocessor1_Model04\n 8   795 roc_auc  binary     0.628    10 0.00914 Preprocessor1_Model04\n 9   838 accuracy binary     0.668    10 0.00481 Preprocessor1_Model05\n10   838 roc_auc  binary     0.628    10 0.00916 Preprocessor1_Model05\n11  1150 accuracy binary     0.669    10 0.00505 Preprocessor1_Model06\n12  1150 roc_auc  binary     0.624    10 0.00914 Preprocessor1_Model06\n13  1239 accuracy binary     0.667    10 0.00530 Preprocessor1_Model07\n14  1239 roc_auc  binary     0.623    10 0.00901 Preprocessor1_Model07\n15  1464 accuracy binary     0.670    10 0.00449 Preprocessor1_Model08\n16  1464 roc_auc  binary     0.622    10 0.00882 Preprocessor1_Model08\n17  1620 accuracy binary     0.666    10 0.00423 Preprocessor1_Model09\n18  1620 roc_auc  binary     0.622    10 0.00904 Preprocessor1_Model09\n19  1930 accuracy binary     0.667    10 0.00420 Preprocessor1_Model10\n20  1930 roc_auc  binary     0.621    10 0.00873 Preprocessor1_Model10\n```\n:::\n:::\n\n\n## Workflow Rec0 - Lasso\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r0_l <- workflow() %>%\n  add_recipe(rec0) %>%\n  add_model(m_l)\n```\n:::\n\n\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2246)\n\nfit_r0_l <- tune_grid(wf_r0_l, cv_folds, grid = lambda_grid, control = control_resamples(save_pred = TRUE))\n```\n:::\n\n\n### Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r0_l_performance <- collect_metrics(fit_r0_l)\n\nwf_r0_l_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.691    10 0.00813 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.644    10 0.00954 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.691    10 0.00813 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.644    10 0.00954 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.691    10 0.00813 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.644    10 0.00954 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.691    10 0.00813 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.644    10 0.00954 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.691    10 0.00813 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.644    10 0.00954 Preprocessor1_Model05\n# ... with 50 more rows\n```\n:::\n:::\n\n\n## Workflow Rec1 - Lasso\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r1_l <- workflow() %>%\n  add_recipe(rec1) %>%\n  add_model(m_l)\n```\n:::\n\n\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2246)\n\nfit_r1_l <- tune_grid(wf_r1_l, cv_folds, grid = lambda_grid, control = control_resamples(save_pred = TRUE))\n```\n:::\n\n\n### Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r1_l_performance <- collect_metrics(fit_r1_l)\n\nwf_r1_l_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.684    10 0.00793 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.644    10 0.00748 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.684    10 0.00793 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.644    10 0.00748 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.684    10 0.00793 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.644    10 0.00748 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.684    10 0.00793 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.644    10 0.00748 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.684    10 0.00793 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.644    10 0.00748 Preprocessor1_Model05\n# ... with 50 more rows\n```\n:::\n:::\n\n\n## Workflow Rec2 - Lasso\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r2_l <- workflow() %>%\n  add_recipe(rec2) %>%\n  add_model(m_l)\n```\n:::\n\n\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2246)\n\nfit_r2_l <- tune_grid(wf_r2_l, cv_folds, grid = lambda_grid, control = control_resamples(save_pred = TRUE))\n```\n:::\n\n\n### Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r2_l_performance <- collect_metrics(fit_r2_l)\n\nwf_r2_l_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.683    10 0.00614 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.647    10 0.00728 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.683    10 0.00614 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.647    10 0.00728 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.683    10 0.00614 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.647    10 0.00728 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.683    10 0.00614 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.647    10 0.00728 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.683    10 0.00614 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.647    10 0.00728 Preprocessor1_Model05\n# ... with 50 more rows\n```\n:::\n:::\n\n\n## Workflow Rec0 - Ridge Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r0_rr <- workflow() %>%\n  add_recipe(rec0) %>%\n  add_model(m_rr)\n```\n:::\n\n\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2246)\n\nfit_r0_rr <- tune_grid(wf_r0_rr, cv_folds, grid = lambda_grid, control = control_resamples(save_pred = TRUE))\n```\n:::\n\n\n### Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r0_rr_performance <- collect_metrics(fit_r0_rr)\n\nwf_r0_rr_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.687    10 0.00693 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.644    10 0.00949 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.687    10 0.00693 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.644    10 0.00949 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.687    10 0.00693 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.644    10 0.00949 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.687    10 0.00693 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.644    10 0.00949 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.687    10 0.00693 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.644    10 0.00949 Preprocessor1_Model05\n# ... with 50 more rows\n```\n:::\n:::\n\n\n## Workflow Rec1 - Ridge Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r1_rr <- workflow() %>%\n  add_recipe(rec1) %>%\n  add_model(m_rr)\n```\n:::\n\n\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2246)\n\nfit_r1_rr <- tune_grid(wf_r1_rr, cv_folds, grid = lambda_grid, control = control_resamples(save_pred = TRUE))\n```\n:::\n\n\n### Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r1_rr_performance <- collect_metrics(fit_r1_rr)\n\nwf_r1_rr_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.685    10 0.00772 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.645    10 0.00752 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.685    10 0.00772 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.645    10 0.00752 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.685    10 0.00772 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.645    10 0.00752 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.685    10 0.00772 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.645    10 0.00752 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.685    10 0.00772 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.645    10 0.00752 Preprocessor1_Model05\n# ... with 50 more rows\n```\n:::\n:::\n\n\n## Workflow Rec2 - Ridge Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r2_rr <- workflow() %>%\n  add_recipe(rec2) %>%\n  add_model(m_rr)\n```\n:::\n\n\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2246)\n\nfit_r2_rr <- tune_grid(wf_r2_rr, cv_folds, grid = lambda_grid, control = control_resamples(save_pred = TRUE))\n```\n:::\n\n\n### Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_r2_rr_performance <- collect_metrics(fit_r2_rr)\n\nwf_r2_rr_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.684    10 0.00581 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.647    10 0.00733 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.684    10 0.00581 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.647    10 0.00733 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.684    10 0.00581 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.647    10 0.00733 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.684    10 0.00581 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.647    10 0.00733 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.684    10 0.00581 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.647    10 0.00733 Preprocessor1_Model05\n# ... with 50 more rows\n```\n:::\n:::\n\n\n# Best Combination\n\nIt seems like the XGBoost model with Recipe 1 gives us the best results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_r1_xgb %>%\n  show_best(\"roc_auc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 x 7\n  trees .metric .estimator  mean     n std_err .config              \n  <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1    23 roc_auc binary     0.661    10 0.00646 Preprocessor1_Model01\n2   223 roc_auc binary     0.648    10 0.00851 Preprocessor1_Model02\n3   459 roc_auc binary     0.643    10 0.00931 Preprocessor1_Model03\n4   795 roc_auc binary     0.639    10 0.00963 Preprocessor1_Model04\n5   838 roc_auc binary     0.639    10 0.00973 Preprocessor1_Model05\n```\n:::\n:::\n\n\nLets save the best one so we can use it for a final fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchosen_auc <- fit_r1_xgb %>% select_best(metric = \"roc_auc\", -penalty)\n```\n:::\n\n\n### Finalize\n\nCreating our final workflow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_final <- finalize_workflow(wf_r1_xgb, chosen_auc)\n\nwf_final\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: boost_tree()\n\n-- Preprocessor ----------------------------------------------------------------\n7 Recipe Steps\n\n* step_tokenize()\n* step_stopwords()\n* step_stem()\n* step_tokenfilter()\n* step_tf()\n* step_zv()\n* step_normalize()\n\n-- Model -----------------------------------------------------------------------\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  trees = 23\n\nEngine-Specific Arguments:\n  nthreads = 12\n\nComputational engine: xgboost \n```\n:::\n:::\n\n\nFinal fit on the train sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_final_train <- fit(wf_final, d_train_id)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[19:14:45] WARNING: amalgamation/../src/learner.cc:627: \nParameters: { \"nthreads\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n```\n:::\n:::\n\n\n## Prediction\n\nLets predict using the test data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_final_test <- fit_final_train %>% \n  predict(d_test_id)\n```\n:::\n\n\nMerge predictions with test data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_final_test_id <- fit_final_test %>% \n  mutate(id = row_number())\n\npredictions <- fit_final_test_id %>% full_join(d_test_id, by = \"id\")\n```\n:::\n\n\nAnd finally lets see the metrics of our prediction, after factorizing the c1 column.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions$c1 <- as.factor(predictions$c1)\n\ntest_metrics <- predictions %>% metrics(c1, .pred_class)\n\nkable(test_metrics)\n```\n\n::: {.cell-output-display}\n|.metric  |.estimator | .estimate|\n|:--------|:----------|---------:|\n|accuracy |binary     | 0.6653454|\n|kap      |binary     | 0.0648416|\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}